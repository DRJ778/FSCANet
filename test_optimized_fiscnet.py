"""
æµ‹è¯•ä¼˜åŒ–åçš„FISCNet Enhanced Correctæ¨¡å‹
åŠ è½½è®­ç»ƒå¥½çš„æƒé‡æ–‡ä»¶ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ¨ç†å’Œè¯„ä¼°

è¯„ä»·æŒ‡æ ‡ï¼šä½¿ç”¨ evaluation_metrics.py ä¸­çš„æ‰€æœ‰21ä¸ªæŒ‡æ ‡è®¡ç®—å‡½æ•°
åŒ…æ‹¬ï¼šCE, NMI, QNCIE, TE, EI, Qy, Qcb, EN, MI, SF, AG, SD, CC, SCD, VIF, MSE, PSNR, Qabf, Nabf, SSIM, MS_SSIM
"""

import torch
import cv2
import numpy as np
import os
import argparse
from pathlib import Path
from tqdm import tqdm
from basicsr.archs import build_network
from basicsr.utils.img_util import tensor2img
import yaml
# ä½¿ç”¨ evaluation_metrics.py ä¸­çš„è¯„ä»·æŒ‡æ ‡å‡½æ•°
from evaluation_metrics import compute_all_metrics, compute_metrics_batch


def load_model(weight_path, device='cuda', arch_type='auto'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        weight_path: æƒé‡æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ ('cuda' or 'cpu')
        arch_type: æ¶æ„ç±»å‹ ('auto', 'FISCNet_DualPath', 'FISCNet_Enhanced_Correct_Optimized')
    """
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡: {weight_path}")
    
    # åŠ è½½æƒé‡æ–‡ä»¶ï¼Œæ£€æŸ¥æ¶æ„ç±»å‹
    checkpoint = torch.load(weight_path, map_location=device, weights_only=False)
    
    # ç¡®å®šæƒé‡é”®å
    if 'params' in checkpoint:
        state_dict = checkpoint['params']
    elif 'params_ema' in checkpoint:
        state_dict = checkpoint['params_ema']
    else:
        state_dict = checkpoint
    
    # è‡ªåŠ¨æ£€æµ‹æ¶æ„ç±»å‹ï¼ˆæ ¹æ®state_dictçš„é”®ï¼‰
    if arch_type == 'auto':
        if 'spatial_branch_vis' in state_dict or 'dual_cafm' in state_dict or 'freq_branch' in state_dict:
            arch_type = 'FISCNet_DualPath'
            print("ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æ¶æ„ç±»å‹: FISCNet_DualPath")
        elif 'ssm_processor' in state_dict and 'sc_with_cafm' in state_dict:
            arch_type = 'FISCNet_Enhanced_Correct_Optimized'
            print("ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æ¶æ„ç±»å‹: FISCNet_Enhanced_Correct_Optimized")
        else:
            # é»˜è®¤ä½¿ç”¨åŒè·¯å¾„æ¶æ„ï¼ˆå¦‚æœè®­ç»ƒä½¿ç”¨çš„æ˜¯æ–°æ¶æ„ï¼‰
            arch_type = 'FISCNet_DualPath'
            print("âš ï¸ æ— æ³•è‡ªåŠ¨æ£€æµ‹æ¶æ„ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨: FISCNet_DualPath")
    
    # æ ¹æ®æ¶æ„ç±»å‹åˆ›å»ºæ¨¡å‹
    if arch_type == 'FISCNet_DualPath':
        network_opt = {
            'type': 'FISCNet_DualPath',
            'vis_channels': 1,
            'inf_channels': 1,
            'n_feat': 16,
            'H': 64,
            'W': 64,
            'num_transformer_layers': 2,
            'num_heads': 4
        }
        print("ğŸ“¦ ä½¿ç”¨ FISCNet_DualPath æ¶æ„")
    elif arch_type == 'FISCNet_Enhanced_Correct_Optimized':
        network_opt = {
            'type': 'FISCNet_Enhanced_Correct_Optimized',
            'vis_channels': 1,
            'inf_channels': 1,
            'n_feat': 16,
            'H': 64,
            'W': 64
        }
        print("ğŸ“¦ ä½¿ç”¨ FISCNet_Enhanced_Correct_Optimized æ¶æ„")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¶æ„ç±»å‹: {arch_type}")
    
    model = build_network(network_opt)
    model.eval()
    model = model.to(device)
    
    # åŠ è½½æƒé‡
    try:
        model.load_state_dict(state_dict, strict=True)
        print("âœ… ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼åŠ è½½æƒé‡æˆåŠŸ")
    except RuntimeError as e:
        print("âš ï¸ ä¸¥æ ¼æ¨¡å¼åŠ è½½å¤±è´¥ï¼Œå°è¯•éä¸¥æ ¼æ¨¡å¼...")
        # å¤„ç†å°ºå¯¸ä¸åŒ¹é…çš„å±‚
        model_state_dict = model.state_dict()
        filtered_state_dict = {}
        skipped_keys = []
        size_mismatch_keys = []
        
        for key, value in state_dict.items():
            if key in model_state_dict:
                if model_state_dict[key].shape == value.shape:
                    filtered_state_dict[key] = value
                else:
                    size_mismatch_keys.append(f"{key}: checkpoint {value.shape} vs model {model_state_dict[key].shape}")
                    print(f"âš ï¸  è·³è¿‡å°ºå¯¸ä¸åŒ¹é…çš„å±‚: {key}")
                    print(f"   checkpointå½¢çŠ¶: {value.shape}, æ¨¡å‹å½¢çŠ¶: {model_state_dict[key].shape}")
            else:
                skipped_keys.append(key)
        
        # åŠ è½½è¿‡æ»¤åçš„æƒé‡
        missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)
        
        if skipped_keys:
            print(f"âš ï¸  æƒé‡æ–‡ä»¶ä¸­å­˜åœ¨ä½†æ¨¡å‹ä¸éœ€è¦çš„é”® ({len(skipped_keys)} ä¸ª):")
            for key in skipped_keys[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   - {key}")
            if len(skipped_keys) > 10:
                print(f"   ... è¿˜æœ‰ {len(skipped_keys) - 10} ä¸ªé”®æœªæ˜¾ç¤º")
        
        if size_mismatch_keys:
            print(f"âš ï¸  å°ºå¯¸ä¸åŒ¹é…çš„å±‚ ({len(size_mismatch_keys)} ä¸ª):")
            for msg in size_mismatch_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {msg}")
            if len(size_mismatch_keys) > 5:
                print(f"   ... è¿˜æœ‰ {len(size_mismatch_keys) - 5} ä¸ªå±‚æœªæ˜¾ç¤º")
        
        if missing_keys:
            print(f"âš ï¸  æ¨¡å‹éœ€è¦ä½†æƒé‡æ–‡ä»¶ä¸­ç¼ºå¤±çš„é”® ({len(missing_keys)} ä¸ª):")
            for key in missing_keys[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   - {key}")
            if len(missing_keys) > 10:
                print(f"   ... è¿˜æœ‰ {len(missing_keys) - 10} ä¸ªé”®æœªæ˜¾ç¤º")
        
        print("âœ… ä½¿ç”¨éä¸¥æ ¼æ¨¡å¼åŠ è½½æƒé‡æˆåŠŸï¼ˆéƒ¨åˆ†å±‚æœªåŠ è½½ï¼‰")
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    return model


def read_image(img_path, grayscale=False):
    """è¯»å–å›¾åƒ"""
    if grayscale:
        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)
        img = img[..., None]  # [H, W, 1]
    else:
        img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    # å½’ä¸€åŒ–åˆ° [0, 1]
    img = img.astype(np.float32) / 255.0
    return img


def img2tensor(img):
    """å°†numpyå›¾åƒè½¬ä¸ºtensor
    Args:
        img: numpy array, shape [H, W, C] (RGB) or [H, W] (grayscale)
    Returns:
        tensor: [1, C, H, W]
    """
    if len(img.shape) == 3:
        img = img.transpose(2, 0, 1)  # [H, W, C] -> [C, H, W]
    else:
        img = np.expand_dims(img, axis=0)  # [H, W] -> [1, H, W]
    
    img = torch.from_numpy(img.copy()).float()
    return img.unsqueeze(0)  # [1, C, H, W]


def tensor2img_np(tensor):
    """å°†tensorè½¬ä¸ºnumpyå›¾åƒï¼ˆå¤„ç†RGBè¾“å‡ºï¼‰"""
    img = tensor.squeeze().cpu().numpy()
    
    # YCrCb2RGBè¾“å‡ºèŒƒå›´åº”è¯¥åœ¨[0,1]ï¼Œå¦‚æœä¸åœ¨åˆ™è£å‰ª
    img = np.clip(img, 0, 1)
    
    # è½¬ä¸º [0, 255]
    if img.max() <= 1.0:
        img = (img * 255).astype(np.uint8)
    else:
        img = np.clip(img, 0, 255).astype(np.uint8)
    
    # å¦‚æœæ˜¯CHWæ ¼å¼ï¼Œè½¬ä¸ºHWC
    if len(img.shape) == 3 and img.shape[0] == 3:
        img = img.transpose(1, 2, 0)
    
    return img


def RGB2YCrCb(input_im, device='cuda'):
    """RGBè½¬YCbCrï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    im_flat = input_im.transpose(1, 3).transpose(1, 2).reshape(-1, 3)  # (nhw,c)
    R = im_flat[:, 0]
    G = im_flat[:, 1]
    B = im_flat[:, 2]
    Y = 0.299 * R + 0.587 * G + 0.114 * B
    Cr = (R - Y) * 0.713 + 0.5
    Cb = (B - Y) * 0.564 + 0.5
    Y = torch.unsqueeze(Y, 1)
    Cr = torch.unsqueeze(Cr, 1)
    Cb = torch.unsqueeze(Cb, 1)
    temp = torch.cat((Y, Cr, Cb), dim=1).to(device)
    out = (
        temp.reshape(
            list(input_im.size())[0],
            list(input_im.size())[2],
            list(input_im.size())[3],
            3,
        )
        .transpose(1, 3)
        .transpose(2, 3)
    )
    return out


def YCrCb2RGB(input_im, device='cuda'):
    """YCbCrè½¬RGBï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    im_flat = input_im.transpose(1, 3).transpose(1, 2).reshape(-1, 3)
    mat = torch.tensor(
        [[1.0, 1.0, 1.0], [1.403, -0.714, 0.0], [0.0, -0.344, 1.773]]
    ).to(device)
    bias = torch.tensor([0.0 / 255, -0.5, -0.5]).to(device)
    temp = (im_flat + bias).mm(mat).to(device)
    out = (
        temp.reshape(
            list(input_im.size())[0],
            list(input_im.size())[2],
            list(input_im.size())[3],
            3,
        )
        .transpose(1, 3)
        .transpose(2, 3)
    )
    return out


def enhance_ir_in_postprocess(fused_y, ir_tensor, enhancement_strength=0.15):
    """
    åå¤„ç†å¢å¼ºï¼šåœ¨èåˆå›¾åƒçš„Yé€šé“ä¸­è¿›ä¸€æ­¥å¢å¼ºçº¢å¤–ä¿¡æ¯
    Args:
        fused_y: æ¨¡å‹è¾“å‡ºçš„èåˆYé€šé“ [B, 1, H, W]
        ir_tensor: çº¢å¤–å›¾åƒ [B, 1, H, W]
        enhancement_strength: å¢å¼ºå¼ºåº¦ (0.0-0.3)ï¼Œæ§åˆ¶çº¢å¤–ä¿¡æ¯çš„é¢å¤–å¢å¼ºå¹…åº¦
    Returns:
        enhanced_y: å¢å¼ºåçš„Yé€šé“
    """
    # ç¡®ä¿å°ºå¯¸ä¸€è‡´
    min_h = min(fused_y.shape[2], ir_tensor.shape[2])
    min_w = min(fused_y.shape[3], ir_tensor.shape[3])
    fused_y = fused_y[:, :, :min_h, :min_w]
    ir_aligned = ir_tensor[:, :, :min_h, :min_w]
    
    # æ–¹æ³•1ï¼šåŸºäºçº¢å¤–æ˜¾è‘—åŒºåŸŸçš„è‡ªé€‚åº”å¢å¼º
    # è®¡ç®—çº¢å¤–å›¾åƒçš„æ˜¾è‘—åº¦ï¼ˆé«˜äº®åº¦åŒºåŸŸï¼‰
    ir_mean = ir_aligned.mean(dim=[2, 3], keepdim=True)
    ir_saliency = (ir_aligned - ir_mean).abs()  # æ˜¾è‘—æ€§çŸ©é˜µ
    ir_saliency_norm = (ir_saliency - ir_saliency.min()) / (ir_saliency.max() - ir_saliency.min() + 1e-8)
    
    # æ–¹æ³•2ï¼šåœ¨çº¢å¤–é«˜äº®åº¦åŒºåŸŸå¢å¼ºèåˆå›¾åƒçš„Yé€šé“
    ir_bright_mask = (ir_aligned > ir_aligned.quantile(0.6)).float()  # é«˜äº60%åˆ†ä½æ•°çš„åŒºåŸŸ
    
    # ç»„åˆå¢å¼ºï¼šåœ¨é«˜æ˜¾è‘—åŒºåŸŸå’Œé«˜äº®åº¦åŒºåŸŸå¢å¼ºçº¢å¤–ä¿¡æ¯
    enhancement_mask = (ir_saliency_norm * 0.6 + ir_bright_mask * 0.4).clamp(0, 1)
    
    # è®¡ç®—å·®å¼‚ï¼šçº¢å¤–å›¾åƒç›¸å¯¹äºèåˆå›¾åƒçš„é¢å¤–ä¿¡æ¯
    ir_delta = (ir_aligned - fused_y).clamp(0, 1)  # åªä¿ç•™çº¢å¤–æ›´å¼ºçš„åœ°æ–¹
    
    # å¢å¼ºèåˆï¼šåœ¨æ˜¾è‘—åŒºåŸŸæ·»åŠ çº¢å¤–çš„é¢å¤–ä¿¡æ¯
    enhanced_y = fused_y + enhancement_strength * enhancement_mask * ir_delta
    
    # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
    enhanced_y = torch.clamp(enhanced_y, 0, 1)
    
    return enhanced_y


def fuse_images_grayscale(model, vis_path, ir_path, device='cuda', postprocess_ir_strength=0.15):
    """
    èåˆå¯è§å…‰å’Œçº¢å¤–å›¾åƒï¼Œç›´æ¥è¾“å‡ºç°åº¦å›¾ï¼ˆä¸å¼•å…¥Yé€šé“çš„Cb/Crï¼Œåªä½¿ç”¨èåˆåçš„Yé€šé“ï¼‰
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        vis_path: å¯è§å…‰å›¾åƒè·¯å¾„
        ir_path: çº¢å¤–å›¾åƒè·¯å¾„
        device: è®¡ç®—è®¾å¤‡
        postprocess_ir_strength: åå¤„ç†çº¢å¤–å¢å¼ºå¼ºåº¦ (0.0=å…³é—­, 0.1-0.3=æ¨èèŒƒå›´)
    Returns:
        fused_gray: èåˆåçš„ç°åº¦å›¾ [H, W] (numpy array, uint8)
        vis_img: åŸå§‹å¯è§å…‰å›¾åƒ [H, W, 3] (å½’ä¸€åŒ–åˆ°[0,1])
        ir_img: åŸå§‹çº¢å¤–å›¾åƒ [H, W, 1] (å½’ä¸€åŒ–åˆ°[0,1])
    """
    # è¯»å–å›¾åƒ
    vis_img = read_image(vis_path, grayscale=False)  # RGB [H, W, 3]
    ir_img = read_image(ir_path, grayscale=True)  # Grayscale [H, W, 1]
    
    # è½¬ä¸ºtensor
    vis_tensor = img2tensor(vis_img).to(device)  # RGBå›¾åƒ [1, 3, H, W]
    ir_tensor = img2tensor(ir_img).to(device)  # ç°åº¦å›¾åƒ [1, 1, H, W]
    
    # ç¡®ä¿å°ºå¯¸ä¸€è‡´
    min_h = min(vis_tensor.shape[2], ir_tensor.shape[2])
    min_w = min(vis_tensor.shape[3], ir_tensor.shape[3])
    vis_tensor = vis_tensor[:, :, :min_h, :min_w]
    ir_tensor = ir_tensor[:, :, :min_h, :min_w]
    
    # RGB -> YCbCr -> æå–Y -> æ¨¡å‹å¤„ç† -> ç›´æ¥è¾“å‡ºYé€šé“ï¼ˆç°åº¦å›¾ï¼‰
    # 1. RGBè½¬YCbCr
    vi_ycrcb = RGB2YCrCb(vis_tensor, device)
    
    # 2. æ¨¡å‹æ¨ç†ï¼ˆè¾“å…¥YCbCrçš„Yé€šé“å’ŒIRï¼‰
    with torch.no_grad():
        outputs = model(vi_ycrcb, ir_tensor)
        # å¤„ç†ä¸‰ä¸ªè¿”å›å€¼ï¼šèåˆå›¾åƒã€å¯è§å…‰é‡å»ºã€çº¢å¤–é‡å»º
        if isinstance(outputs, tuple) and len(outputs) == 3:
            output_y, vis_recon, ir_recon = outputs
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šå¦‚æœæ¨¡å‹åªè¿”å›ä¸€ä¸ªå€¼
            output_y = outputs
        output_y = torch.clamp(output_y, 0, 1)  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
    
    # åå¤„ç†ï¼šè¿›ä¸€æ­¥å¢å¼ºçº¢å¤–ä¿¡æ¯
    if postprocess_ir_strength > 0:
        output_y = enhance_ir_in_postprocess(output_y, ir_tensor, postprocess_ir_strength)
    
    # 3. ç›´æ¥å°†èåˆåçš„Yé€šé“è½¬ä¸ºç°åº¦å›¾ï¼ˆä¸åˆå¹¶Cb/Crï¼Œä¸è½¬å›RGBï¼‰
    # å°†tensorè½¬ä¸ºnumpy
    fused_y_np = output_y.squeeze().cpu().numpy()  # [H, W]
    
    # ç¡®ä¿åœ¨[0, 1]èŒƒå›´å†…
    fused_y_np = np.clip(fused_y_np, 0, 1)
    
    # è½¬ä¸º [0, 255] çš„uint8æ ¼å¼
    fused_gray = (fused_y_np * 255).astype(np.uint8)
    
    return fused_gray, vis_img, ir_img


def fuse_images(model, vis_path, ir_path, device='cuda', postprocess_ir_strength=0.15):
    """
    èåˆå¯è§å…‰å’Œçº¢å¤–å›¾åƒï¼ˆå®Œå…¨æŒ‰ç…§è®­ç»ƒæ—¶çš„æµç¨‹ + åå¤„ç†å¢å¼ºï¼‰
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        vis_path: å¯è§å…‰å›¾åƒè·¯å¾„
        ir_path: çº¢å¤–å›¾åƒè·¯å¾„
        device: è®¡ç®—è®¾å¤‡
        postprocess_ir_strength: åå¤„ç†çº¢å¤–å¢å¼ºå¼ºåº¦ (0.0=å…³é—­, 0.1-0.3=æ¨èèŒƒå›´)
    """
    # è¯»å–å›¾åƒ
    vis_img = read_image(vis_path, grayscale=False)  # RGB [H, W, 3]
    ir_img = read_image(ir_path, grayscale=True)  # Grayscale [H, W, 1]
    
    # è½¬ä¸ºtensor
    vis_tensor = img2tensor(vis_img).to(device)  # RGBå›¾åƒ [1, 3, H, W]
    ir_tensor = img2tensor(ir_img).to(device)  # ç°åº¦å›¾åƒ [1, 1, H, W]
    
    # ç¡®ä¿å°ºå¯¸ä¸€è‡´
    min_h = min(vis_tensor.shape[2], ir_tensor.shape[2])
    min_w = min(vis_tensor.shape[3], ir_tensor.shape[3])
    vis_tensor = vis_tensor[:, :, :min_h, :min_w]
    ir_tensor = ir_tensor[:, :, :min_h, :min_w]
    
    # ã€å…³é”®ä¿®å¤ã€‘æŒ‰ç…§è®­ç»ƒæ—¶çš„æµç¨‹ï¼šRGB -> YCbCr -> æå–Y -> æ¨¡å‹å¤„ç† -> åˆå¹¶Yå’ŒCrCb -> YCbCr2RGB
    # 1. RGBè½¬YCbCr
    vi_ycrcb = RGB2YCrCb(vis_tensor, device)
    
    # 2. æ¨¡å‹æ¨ç†ï¼ˆè¾“å…¥YCbCrçš„Yé€šé“å’ŒIRï¼‰
    with torch.no_grad():
        outputs = model(vi_ycrcb, ir_tensor)
        # ã€ä¿®æ”¹ã€‘å¤„ç†ä¸‰ä¸ªè¿”å›å€¼ï¼šèåˆå›¾åƒã€å¯è§å…‰é‡å»ºã€çº¢å¤–é‡å»º
        if isinstance(outputs, tuple) and len(outputs) == 3:
            output_y, vis_recon, ir_recon = outputs
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šå¦‚æœæ¨¡å‹åªè¿”å›ä¸€ä¸ªå€¼
            output_y = outputs
        output_y = torch.clamp(output_y, 0, 1)  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
    
    # ã€æ–°å¢ã€‘åå¤„ç†ï¼šè¿›ä¸€æ­¥å¢å¼ºçº¢å¤–ä¿¡æ¯
    if postprocess_ir_strength > 0:
        output_y = enhance_ir_in_postprocess(output_y, ir_tensor, postprocess_ir_strength)
    
    # 3. å¯¹é½å°ºå¯¸ï¼ˆåŒè®­ç»ƒæ—¶ï¼‰
    oh, ow = output_y.shape[-2], output_y.shape[-1]
    ch, cw = vi_ycrcb.shape[-2], vi_ycrcb.shape[-3]
    if (oh != ch) or (ow != cw):
        top = max((ch - oh) // 2, 0)
        left = max((cw - ow) // 2, 0)
        c1 = vi_ycrcb[:, 1:2, top:top+oh, left:left+ow]
        c2 = vi_ycrcb[:, 2:, top:top+oh, left:left+ow]
    else:
        c1 = vi_ycrcb[:, 1:2, :, :]
        c2 = vi_ycrcb[:, 2:, :, :]
    
    # 4. åˆå¹¶Yå’ŒCrCb
    output_ycrcb = torch.cat((output_y, c1, c2), dim=1)
    
    # 5. YCbCrè½¬RGB
    output_rgb = YCrCb2RGB(output_ycrcb, device)
    
    # è½¬ä¸ºnumpy
    fused_img = tensor2img_np(output_rgb)
    
    return fused_img, vis_img, ir_img


def test_on_dataset(model, vis_dir, ir_dir, output_dir, device='cuda', compute_metrics=True, 
                   postprocess_ir_strength=0.15):
    """åœ¨æ•°æ®é›†ä¸Šæµ‹è¯•å¹¶è®¡ç®—è¯„ä»·æŒ‡æ ‡"""
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–å›¾åƒåˆ—è¡¨
    vis_files = sorted(list(Path(vis_dir).glob('*.png')) + list(Path(vis_dir).glob('*.jpg')))
    ir_files = sorted(list(Path(ir_dir).glob('*.png')) + list(Path(ir_dir).glob('*.jpg')))
    
    if len(vis_files) != len(ir_files):
        print(f"âš ï¸  å¯è§å…‰å›¾åƒ({len(vis_files)})å’Œçº¢å¤–å›¾åƒ({len(ir_files)})æ•°é‡ä¸åŒ¹é…")
        min_len = min(len(vis_files), len(ir_files))
        vis_files = vis_files[:min_len]
        ir_files = ir_files[:min_len]
    
    print(f"æ‰¾åˆ° {len(vis_files)} å¯¹å›¾åƒ")
    
    results = []
    all_metrics_list = []
    detailed_metrics = []  # å­˜å‚¨æ¯å¼ å›¾åƒçš„è¯¦ç»†æŒ‡æ ‡
    
    # å®šä¹‰æ‰€æœ‰æŒ‡æ ‡åç§°
    metric_names = ['CE', 'NMI', 'QNCIE', 'TE', 'EI', 'Qy', 'Qcb', 'EN', 'MI', 
                   'SF', 'AG', 'SD', 'CC', 'SCD', 'VIF', 'MSE', 'PSNR', 
                   'Qabf', 'Nabf', 'SSIM', 'MS_SSIM']
    
    for idx, (vis_path, ir_path) in enumerate(tqdm(zip(vis_files, ir_files), total=len(vis_files))):
        try:
            # èåˆå›¾åƒï¼ˆä½¿ç”¨åå¤„ç†å¢å¼ºï¼‰
            fused_img, vis_img_norm, ir_img_norm = fuse_images(model, vis_path, ir_path, device, 
                                                              postprocess_ir_strength=postprocess_ir_strength)
            
            # ä¿å­˜ç»“æœï¼ˆè½¬ä¸ºBGRä¿å­˜ï¼‰
            output_path = os.path.join(output_dir, f"{vis_path.stem}_fused.png")
            cv2.imwrite(output_path, cv2.cvtColor(fused_img, cv2.COLOR_RGB2BGR))
            
            # è®¡ç®—è¯„ä»·æŒ‡æ ‡ï¼ˆä½¿ç”¨ evaluation_metrics.py ä¸­çš„å‡½æ•°ï¼‰
            if compute_metrics:
                # è¯»å–åŸå§‹å›¾åƒç”¨äºè¯„ä»·ï¼ˆéœ€è¦è½¬æ¢ä¸ºuint8æ ¼å¼ï¼‰
                vis_img_uint8 = (vis_img_norm * 255).astype(np.uint8)
                ir_img_uint8 = (ir_img_norm.squeeze() * 255).astype(np.uint8)
                if len(ir_img_uint8.shape) == 2:
                    ir_img_uint8 = np.stack([ir_img_uint8] * 3, axis=2)  # è½¬ä¸º3é€šé“
                
                # è½¬ä¸ºBGRæ ¼å¼ï¼ˆevaluation_metrics.py ä½¿ç”¨ cv2.imreadï¼Œé»˜è®¤æ˜¯BGRæ ¼å¼ï¼‰
                fused_img_bgr = cv2.cvtColor(fused_img, cv2.COLOR_RGB2BGR)
                vis_img_bgr = cv2.cvtColor(vis_img_uint8, cv2.COLOR_RGB2BGR)
                
                try:
                    # ä½¿ç”¨ evaluation_metrics.py ä¸­çš„ compute_all_metrics å‡½æ•°
                    # è®¡ç®—æ‰€æœ‰21ä¸ªè¯„ä»·æŒ‡æ ‡ï¼šCE, NMI, QNCIE, TE, EI, Qy, Qcb, EN, MI, SF, AG, SD, CC, SCD, VIF, MSE, PSNR, Qabf, Nabf, SSIM, MS_SSIM
                    metrics = compute_all_metrics(fused_img_bgr, vis_img_bgr, ir_img_uint8)
                    all_metrics_list.append(metrics)
                    
                    # ä¿å­˜è¯¥å›¾åƒçš„æŒ‡æ ‡ä¿¡æ¯
                    image_metrics = {
                        'image_name': vis_path.stem,
                        'metrics': metrics.copy()
                    }
                    detailed_metrics.append(image_metrics)
                    
                except Exception as e:
                    print(f"âš ï¸  è®¡ç®— {vis_path.stem} çš„æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
            
            results.append({
                'name': vis_path.stem,
                'vis_path': str(vis_path),
                'ir_path': str(ir_path),
                'fused_path': output_path
            })
            
        except Exception as e:
            print(f"âŒ å¤„ç† {vis_path.name} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼èåˆç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"   æ€»å…±å¤„ç†äº† {len(results)} å¯¹å›¾åƒ")
    
    # è®¡ç®—å¹¶æ˜¾ç¤ºå¹³å‡è¯„ä»·æŒ‡æ ‡ï¼ˆåŸºäº evaluation_metrics.py çš„è®¡ç®—ç»“æœï¼‰
    if compute_metrics and all_metrics_list:
        print("\n" + "="*60)
        print("è¯„ä»·æŒ‡æ ‡ç»“æœï¼ˆå¹³å‡å€¼ï¼‰- ä½¿ç”¨ evaluation_metrics.py")
        print("="*60)
        print(f"è®¡ç®—äº† {len(all_metrics_list)} å¼ å›¾åƒçš„æŒ‡æ ‡\n")
        
        avg_metrics = {}
        # evaluation_metrics.py æ”¯æŒçš„æ‰€æœ‰21ä¸ªæŒ‡æ ‡ï¼ˆæŒ‰ç…§è¯„ä»·æŒ‡æ ‡æ–‡ä»¶å¤¹ä¸­çš„é¡ºåºï¼‰
        # ç§»é™¤SSIM_aï¼Œå› ä¸ºå®ƒä¸åœ¨è¯„ä»·æŒ‡æ ‡æ–‡ä»¶å¤¹ä¸­
        metric_names = ['CE', 'NMI', 'QNCIE', 'TE', 'EI', 'Qy', 'Qcb', 'EN', 'MI', 
                       'SF', 'AG', 'SD', 'CC', 'SCD', 'VIF', 'MSE', 'PSNR', 
                       'Qabf', 'Nabf', 'SSIM', 'MS_SSIM']
        
        # è®¡ç®—å¹³å‡å€¼
        for metric_name in metric_names:
            values = [m[metric_name] for m in all_metrics_list 
                     if metric_name in m and not np.isnan(m[metric_name])]
            if values:
                avg_metrics[metric_name] = np.mean(values)
                print(f"{metric_name:10}: {avg_metrics[metric_name]:.4f}")
            else:
                avg_metrics[metric_name] = np.nan
                print(f"{metric_name:10}: N/A")
        
        # ä¿å­˜æŒ‡æ ‡åˆ°æ–‡ä»¶ï¼ˆåŒ…å«æ¯å¼ å›¾åƒçš„è¯¦ç»†æŒ‡æ ‡å’Œå¹³å‡å€¼ï¼‰
        metrics_file = os.path.join(output_dir, "metrics_results.txt")
        with open(metrics_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("è¯„ä»·æŒ‡æ ‡ç»“æœ - ä½¿ç”¨ evaluation_metrics.py\n")
            f.write("="*80 + "\n")
            f.write(f"å¤„ç†å›¾åƒæ•°: {len(all_metrics_list)}\n\n")
            
            # å†™å…¥æŒ‡æ ‡è¯´æ˜
            f.write("æŒ‡æ ‡è¯´æ˜ï¼š\n")
            f.write("  CE: äº¤å‰ç†µ (Cross Entropy)\n")
            f.write("  NMI: å½’ä¸€åŒ–äº’ä¿¡æ¯ (Normalized Mutual Information)\n")
            f.write("  QNCIE: å½’ä¸€åŒ–äº’ç›¸å…³ç†µ (Normalized Cross-Correlation Entropy)\n")
            f.write("  TE: æ€»ç†µ (Total Entropy)\n")
            f.write("  EI: è¾¹ç¼˜å¼ºåº¦ (Edge Intensity)\n")
            f.write("  Qy: åŸºäºSSIMçš„èåˆè´¨é‡æŒ‡æ ‡\n")
            f.write("  Qcb: åŸºäºå¯¹æ¯”åº¦çš„èåˆè´¨é‡æŒ‡æ ‡\n")
            f.write("  EN: ä¿¡æ¯ç†µ (Entropy)\n")
            f.write("  MI: äº’ä¿¡æ¯ (Mutual Information)\n")
            f.write("  SF: ç©ºé—´é¢‘ç‡ (Spatial Frequency)\n")
            f.write("  AG: å¹³å‡æ¢¯åº¦ (Average Gradient)\n")
            f.write("  SD: æ ‡å‡†å·® (Standard Deviation)\n")
            f.write("  CC: ç›¸å…³ç³»æ•° (Correlation Coefficient)\n")
            f.write("  SCD: å·®å¼‚ç›¸å…³å’Œ (Sum of Correlation of Differences)\n")
            f.write("  VIF: è§†è§‰ä¿¡æ¯ä¿çœŸåº¦ (Visual Information Fidelity)\n")
            f.write("  MSE: å‡æ–¹è¯¯å·® (Mean Squared Error)\n")
            f.write("  PSNR: å³°å€¼ä¿¡å™ªæ¯” (Peak Signal-to-Noise Ratio)\n")
            f.write("  Qabf: è¾¹ç¼˜è´¨é‡æŒ‡æ ‡ (Edge-based Quality)\n")
            f.write("  Nabf: åŸºäºè¾¹ç¼˜çš„èåˆè´¨é‡æŒ‡æ ‡ (Negative Artifact-based Fusion)\n")
            f.write("  SSIM: ç»“æ„ç›¸ä¼¼æ€§ (Structural Similarity Index)\n")
            f.write("  MS_SSIM: å¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§ (Multi-Scale SSIM)\n\n")
            
            # å†™å…¥æ¯å¼ å›¾åƒçš„è¯¦ç»†æŒ‡æ ‡
            f.write("\n" + "="*80 + "\n")
            f.write("æ¯å¼ å›¾åƒçš„è¯¦ç»†æŒ‡æ ‡\n")
            f.write("="*80 + "\n\n")
            
            for img_metrics in detailed_metrics:
                f.write(f"å›¾åƒåç§°: {img_metrics['image_name']}\n")
                f.write("-" * 80 + "\n")
                for metric_name in metric_names:
                    if metric_name in img_metrics['metrics'] and not np.isnan(img_metrics['metrics'][metric_name]):
                        f.write(f"{metric_name:10}: {img_metrics['metrics'][metric_name]:.4f}\n")
                    else:
                        f.write(f"{metric_name:10}: N/A\n")
                f.write("\n")
            
            # å†™å…¥å¹³å‡å€¼
            f.write("\n" + "="*80 + "\n")
            f.write("å¹³å‡å€¼ï¼ˆæ‰€æœ‰å›¾åƒï¼‰\n")
            f.write("="*80 + "\n")
            for metric_name in metric_names:
                if not np.isnan(avg_metrics[metric_name]):
                    f.write(f"{metric_name:10}: {avg_metrics[metric_name]:.4f}\n")
                else:
                    f.write(f"{metric_name:10}: N/A\n")
        
        print(f"\nâœ… æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")
        print("="*60)
        
        return results, avg_metrics
    else:
        return results, None


def test_single_pair(model, vis_path, ir_path, output_path=None, device='cuda', compute_metrics=True,
                    postprocess_ir_strength=0.15):
    """æµ‹è¯•å•å¯¹å›¾åƒå¹¶è®¡ç®—è¯„ä»·æŒ‡æ ‡"""
    print(f"å¯è§å…‰å›¾åƒ: {vis_path}")
    print(f"çº¢å¤–å›¾åƒ: {ir_path}")
    print(f"åå¤„ç†çº¢å¤–å¢å¼ºå¼ºåº¦: {postprocess_ir_strength}")
    
    fused_img, vis_img_norm, ir_img_norm = fuse_images(model, vis_path, ir_path, device, 
                                                       postprocess_ir_strength=postprocess_ir_strength)
    
    if output_path is None:
        output_path = "fused_result.png"
    
    cv2.imwrite(output_path, cv2.cvtColor(fused_img, cv2.COLOR_RGB2BGR))
    print(f"âœ… èåˆç»“æœå·²ä¿å­˜: {output_path}")
    
    # è®¡ç®—è¯„ä»·æŒ‡æ ‡ï¼ˆä½¿ç”¨ evaluation_metrics.pyï¼‰
    if compute_metrics:
        try:
            # è½¬æ¢ä¸ºuint8æ ¼å¼
            vis_img_uint8 = (vis_img_norm * 255).astype(np.uint8)
            ir_img_uint8 = (ir_img_norm.squeeze() * 255).astype(np.uint8)
            if len(ir_img_uint8.shape) == 2:
                ir_img_uint8 = np.stack([ir_img_uint8] * 3, axis=2)  # è½¬ä¸º3é€šé“
            
            # è½¬ä¸ºBGRæ ¼å¼ï¼ˆevaluation_metrics.py ä½¿ç”¨ cv2.imreadï¼Œé»˜è®¤æ˜¯BGRæ ¼å¼ï¼‰
            fused_img_bgr = cv2.cvtColor(fused_img, cv2.COLOR_RGB2BGR)
            vis_img_bgr = cv2.cvtColor(vis_img_uint8, cv2.COLOR_RGB2BGR)
            
            # ä½¿ç”¨ evaluation_metrics.py ä¸­çš„ compute_all_metrics å‡½æ•°
            metrics = compute_all_metrics(fused_img_bgr, vis_img_bgr, ir_img_uint8)
            
            print("\n" + "="*60)
            print("è¯„ä»·æŒ‡æ ‡ç»“æœ - ä½¿ç”¨ evaluation_metrics.py")
            print("="*60)
            metric_names = ['EN', 'SD', 'SF', 'AG', 'PSNR', 'SSIM', 'SSIM_a', 'CC', 'MI', 'SCD', 'VIF', 'Qabf']
            for metric_name in metric_names:
                if metric_name in metrics and not np.isnan(metrics[metric_name]):
                    print(f"{metric_name:8}: {metrics[metric_name]:.4f}")
                else:
                    print(f"{metric_name:8}: N/A")
            print("="*60)
        except Exception as e:
            print(f"âš ï¸  è®¡ç®—è¯„ä»·æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    return fused_img


def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•FISCNetæ¨¡å‹ï¼ˆæ”¯æŒåŒè·¯å¾„æ¶æ„å’ŒåŸæ¶æ„ï¼‰')
    parser.add_argument('--weight', type=str, required=True, help='æƒé‡æ–‡ä»¶è·¯å¾„ (net_g_50000.pth)')
    parser.add_argument('--vis_dir', type=str, help='å¯è§å…‰å›¾åƒç›®å½•')
    parser.add_argument('--ir_dir', type=str, help='çº¢å¤–å›¾åƒç›®å½•')
    parser.add_argument('--vis_img', type=str, help='å•å¼ å¯è§å…‰å›¾åƒè·¯å¾„')
    parser.add_argument('--ir_img', type=str, help='å•å¼ çº¢å¤–å›¾åƒè·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda', help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--arch', type=str, default='auto', 
                       choices=['auto', 'FISCNet_DualPath', 'FISCNet_Enhanced_Correct_Optimized'],
                       help='æ¶æ„ç±»å‹ï¼ˆautoè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰')
    parser.add_argument('--no_metrics', action='store_true', help='ä¸è®¡ç®—è¯„ä»·æŒ‡æ ‡ï¼ˆä»…èåˆå›¾åƒï¼‰')
    parser.add_argument('--ir_enhance', type=float, default=0.15, 
                       help='åå¤„ç†çº¢å¤–å¢å¼ºå¼ºåº¦ (0.0=å…³é—­, 0.1-0.3=æ¨èèŒƒå›´, é»˜è®¤0.15)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è®¾å¤‡
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        args.device = 'cpu'
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.weight, args.device, arch_type=args.arch)
    
    # æ˜¯å¦è®¡ç®—æŒ‡æ ‡
    compute_metrics = not args.no_metrics
    
    # æµ‹è¯•æ¨¡å¼é€‰æ‹©
    if args.vis_img and args.ir_img:
        # å•å¯¹å›¾åƒæµ‹è¯•
        test_single_pair(model, args.vis_img, args.ir_img, 
                        output_path=args.output_dir, device=args.device, 
                        compute_metrics=compute_metrics,
                        postprocess_ir_strength=args.ir_enhance)
    elif args.vis_dir and args.ir_dir:
        # æ•°æ®é›†æµ‹è¯•
        results, metrics = test_on_dataset(model, args.vis_dir, args.ir_dir, 
                       args.output_dir, device=args.device, 
                       compute_metrics=compute_metrics,
                       postprocess_ir_strength=args.ir_enhance)
        if metrics:
            return results, metrics
    else:
        print("âŒ è¯·æä¾› --vis_dir å’Œ --ir_dir (æ•°æ®é›†æµ‹è¯•) æˆ– --vis_img å’Œ --ir_img (å•å¯¹å›¾åƒæµ‹è¯•)")
        parser.print_help()


if __name__ == '__main__':
    main()
